defaults:
  - dataset: cover_dataset
  - model: gan
  - _self_

optimizer:
  generator:
    cls: torch.optim.AdamW
    kwargs:
      lr: 3e-4
      weight_decay: 0.01
  discriminator:
    cls: torch.optim.AdamW
    kwargs:
      lr: 3e-4
      weight_decay: 0.01

logger: {}

trainer:
  max_steps: 10000
  gpus: 0
  amp_backend: "apex"
  auto_lr_find: True
  auto_scale_batch_size: binsearch
  gradient_clip_val: 0.5
  enable_progress_bar: True
  callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      dirpath: "${hydra:runtime.cwd}/checkpoints/"
      filename: "{epoch}-{generator_loss}-{discriminator_loss}"
      monitor: "generator_loss"
      every_n_train_steps: 10000
      save_top_k: 1
      save_weights_only: True
    - _target_: pytorch_lightning.callbacks.EarlyStopping
      monitor: "generator_loss"
      patience: 3
      verbose: True
      mode: min
